{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "from glob import glob\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import codecs\n",
    "import jinja2\n",
    "import json\n",
    "import os\n",
    "\n",
    "class StemTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.ignore_set = {'placeholder','appliance','toward','available'}\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        words = []\n",
    "        for word in word_tokenize(doc):\n",
    "            word = word.lower()\n",
    "            w = wn.morphy(word)\n",
    "            if w and len(w) > 3 and w not in self.ignore_set:\n",
    "                words.append(w)\n",
    "        return words\n",
    "\n",
    "class StemTokenizerRemoveDoubles(object):\n",
    "    def __init__(self):\n",
    "        self.ignore_set = {'placeholder','appliance','toward','available'}\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        words = []\n",
    "        for word in word_tokenize(doc):\n",
    "            word = word.lower()\n",
    "            w = wn.morphy(word)\n",
    "            if w and len(w) > 3 and w not in self.ignore_set and w not in words:\n",
    "                words.append(w)\n",
    "        return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/python3/3.5.2_3/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (8,10,13,19,20,23,31,34,35,36,37,38,39,40,41,42,43,44,45,46,47,49,50,51,53,54,55,57,58,59,61,62,63,65,66,67) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "grants_df = pd.read_csv(\"data/grants.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_year(x):\n",
    "  m = x.split('-')\n",
    "  return int(m[0])\n",
    "\n",
    "grants_df['year'] = grants_df['Award Date'].apply(get_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1991        1\n",
       "1996        3\n",
       "1997      140\n",
       "1998      337\n",
       "1999      454\n",
       "2000      392\n",
       "2001      482\n",
       "2002      448\n",
       "2003      447\n",
       "2004    18725\n",
       "2005    25966\n",
       "2006    21012\n",
       "2007    15606\n",
       "2008    13927\n",
       "2009    16494\n",
       "2010    15930\n",
       "2011    16029\n",
       "2012    20833\n",
       "2013    19328\n",
       "2014    22805\n",
       "2015    22387\n",
       "2016    25121\n",
       "2017    26027\n",
       "2018     1054\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.value_counts(grants_df['year'].values).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "grants_df2 = grants_df.loc[grants_df.Description.apply(type) != float]\n",
    "grants_df3 = grants_df2[(grants_df2['year'] > 2004) & (grants_df2['year'] < 2018)]\n",
    "\n",
    "def get_raw_data():\n",
    "  grouped_df = grants_df3.groupby('year')\n",
    "  indices = list(grouped_df.indices.keys())\n",
    "  indices.sort()\n",
    "  print(indices)\n",
    "#  return list(zip(indices,grouped_df['Description'].apply('\\n'.join).tolist()))\n",
    "  return list(zip(indices,grouped_df['description_strings'].apply('\\n'.join).tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017]\n"
     ]
    }
   ],
   "source": [
    "grouped_df = grants_df3.groupby('year')\n",
    "indices = list(grouped_df.indices.keys())\n",
    "indices.sort()\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/python3/3.5.2_3/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/usr/local/Cellar/python3/3.5.2_3/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "tokenizer = StemTokenizerRemoveDoubles()\n",
    "grants_df3['token_descriptions'] = grants_df3['Description'].apply(tokenizer)\n",
    "grants_df3['description_strings'] = grants_df3['token_descriptions'].apply(' '.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearmask = grants_df3['year'] == 2008\n",
    "token_mask = grants_df3['token_descriptions'].apply((lambda x, key: True if key in x else False),args=('horse',))\n",
    "size = len(grants_df3[(yearmask) & (token_mask)])\n",
    "dict = grants_df3[(yearmask) & (token_mask)].sample(5 if size > 5 else size)[['Title','Identifier','Amount Awarded','Funding Org:Name','Recipient Org:Name']].to_dict()\n",
    "dict['size'] = size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def award_data(keywords,year):\n",
    "  print('data')\n",
    "  dict = {}\n",
    "  yearmask = grants_df3['year'] == year\n",
    "  for idx,kw in enumerate(keywords):\n",
    "    dict[kw] = get_grant_data(grants_df3,kw,yearmask)\n",
    "    if(idx % 100 == 0): print(idx)        \n",
    "  return dict\n",
    "\n",
    "def award_values(keywords,year):\n",
    "  print('award values')\n",
    "  dict = {}\n",
    "  yearmask = grants_df3['year'] == year\n",
    "  for idx,kw in enumerate(keywords):\n",
    "    dict[kw] = get_amount_awarded(grants_df3,kw,yearmask)\n",
    "    if(idx % 100 == 0): print(idx)        \n",
    "  return dict\n",
    "\n",
    "def beneficiaries(keywords,year):\n",
    "  print('benefs')\n",
    "  dict = {}\n",
    "  yearmask = grants_df3['year'] == year\n",
    "  for idx,kw in enumerate(keywords):\n",
    "    dict[kw] = get_beneficiary(grants_df3,kw,yearmask)\n",
    "    if(idx % 100 == 0): print(idx)        \n",
    "  return dict\n",
    "\n",
    "def get_mask(df,keyword,yearmask):\n",
    "  token_mask = grants_df3['token_descriptions'].apply((lambda x, key: True if key in x else False),args=(keyword,))\n",
    "  return (token_mask) & (yearmask)\n",
    "\n",
    "def get_amount_awarded(df,keyword,yearmask):\n",
    "  token_mask = grants_df3['token_descriptions'].apply((lambda x, key: True if key in x else False),args=(keyword,))\n",
    "#  mask = get_mask(df,keyword,yearmask)\n",
    "  return df[(yearmask) & (token_mask)]['Amount Awarded'].sum()\n",
    "\n",
    "def get_beneficiary(df,keyword,yearmask):\n",
    "  token_mask = grants_df3['token_descriptions'].apply((lambda x, key: True if key in x else False),args=(keyword,))\n",
    "#  mask = get_mask(df,keyword,yearmask)\n",
    "  return df[(yearmask) & (token_mask)]['Funding Org:Name'].value_counts().to_dict()    \n",
    "\n",
    "def get_grant_data(df,keyword,yearmask):\n",
    "  token_mask = grants_df3['token_descriptions'].apply((lambda x, key: True if key in x else False),args=(keyword,))\n",
    "  size = len(grants_df3[(yearmask) & (token_mask)])\n",
    "  dict = grants_df3[(yearmask) & (token_mask)].sample(5 if size > 5 else size)[['Title','Identifier','Amount Awarded','Funding Org:Name','Recipient Org:Name']].to_dict()\n",
    "  dict['size'] = size\n",
    "  return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(counts, vectorizer, text, year, index):\n",
    "    result = {w: counts[index][vectorizer.vocabulary_.get(w)]\n",
    "              for w in vectorizer.get_feature_names()}\n",
    "\n",
    "    result = {w: c for w, c in result.items() if c > 4}\n",
    "    normalizing_factor = max(c for c in result.values())\n",
    "\n",
    "    result = {w: c / normalizing_factor\n",
    "              for w, c in result.items()}\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017]\n",
      "Data loaded\n",
      "Vectorization done.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "data = list(get_raw_data())\n",
    "print('Data loaded')\n",
    "n = len(data)\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english',\n",
    "                             max_df=(n-1) / n,\n",
    "                             tokenizer=StemTokenizer())\n",
    "\n",
    "tfids = vectorizer.fit_transform(text for p, text in data).toarray()\n",
    "\n",
    "print('Vectorization done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spouse', 0.8686458800117584),\n",
       " ('60th', 0.37118179864504475),\n",
       " ('malta', 0.17968260495918828),\n",
       " ('commemoration', 0.11013677743733379),\n",
       " ('holland', 0.09648384923770248),\n",
       " ('wartime', 0.08152991457816042),\n",
       " ('egypt', 0.07564146432955801),\n",
       " ('singapore', 0.07523902610762334),\n",
       " ('memorabilia', 0.07051552971112636),\n",
       " ('legion', 0.06794159548180034),\n",
       " ('reunion', 0.06627410032589758),\n",
       " ('burma', 0.057639148879980576),\n",
       " ('gibraltar', 0.03638403543245197),\n",
       " ('tunisia', 0.03332369600355414),\n",
       " ('remembrance', 0.033254142212994546),\n",
       " ('evacuee', 0.03192554504880753),\n",
       " ('1940s', 0.029944629119756445),\n",
       " ('sicily', 0.027034777724644254),\n",
       " ('jersey', 0.025162790859826596),\n",
       " ('victory', 0.021762413716606786),\n",
       " ('norway', 0.021137385261004553),\n",
       " ('trafalgar', 0.01862631571282927),\n",
       " ('russia', 0.01826635980713785),\n",
       " ('postage', 0.01803217570704634),\n",
       " ('naval', 0.017589411630041022),\n",
       " ('greece', 0.016158702906314253),\n",
       " ('cyprus', 0.014621621715845185),\n",
       " ('raid', 0.01440099890366156),\n",
       " ('normandy', 0.013816861905399143),\n",
       " ('photocopier', 0.013588319096360067),\n",
       " ('regiment', 0.013259710305723232),\n",
       " ('buffet', 0.012645941404941588),\n",
       " ('stationary', 0.011006652704301012),\n",
       " ('crete', 0.010541169143981411),\n",
       " ('libya', 0.010336231044065435),\n",
       " ('pigeon', 0.010317057091680792),\n",
       " ('cupboard', 0.010069916303934967),\n",
       " ('remember', 0.010069916303934967),\n",
       " ('fancy', 0.009861093715337449),\n",
       " ('mediterranean', 0.009813786014037828),\n",
       " ('remembering', 0.00966624134447082),\n",
       " ('worcester', 0.009601548103751946),\n",
       " ('malaysia', 0.009562150475216344),\n",
       " ('soloist', 0.009292731075762705),\n",
       " ('tape', 0.008807243858751895),\n",
       " ('thanksgiving', 0.008490475500482321),\n",
       " ('israel', 0.008430627603294393),\n",
       " ('creche', 0.008430627603294393),\n",
       " ('flypast', 0.008308268598266284),\n",
       " ('aircraft', 0.008118189983095858),\n",
       " ('clocks', 0.00796225940311137),\n",
       " ('spitfire', 0.007754384025048532),\n",
       " ('drumhead', 0.007754384025048532),\n",
       " ('regimental', 0.007728075303019858),\n",
       " ('folk', 0.007728075303019858),\n",
       " ('choral', 0.007576977317556133),\n",
       " ('atmosphere', 0.0074938912029283485),\n",
       " ('nostalgic', 0.007480829715083583),\n",
       " ('borrow', 0.007025523002745326),\n",
       " ('navy', 0.007025523002745326),\n",
       " ('rationing', 0.006876271241281621),\n",
       " ('copier', 0.00680075428643962),\n",
       " ('photocopy', 0.006794159548180034),\n",
       " ('aviation', 0.006794159548180034),\n",
       " ('reimburse', 0.006637665054116218),\n",
       " ('evacuate', 0.006637665054116218),\n",
       " ('ex-serviceman', 0.006557154802562305),\n",
       " ('vintage', 0.006557154802562305),\n",
       " ('ration', 0.006444160896313881),\n",
       " ('collate', 0.006322970702470794),\n",
       " ('bulb', 0.006290888470537068),\n",
       " ('airfield', 0.006275568848182585),\n",
       " ('parachute', 0.006088786602379283),\n",
       " ('austria', 0.005972799729863265),\n",
       " ('umpire', 0.005953339320936962),\n",
       " ('marines', 0.005658441849344146),\n",
       " ('spiritual', 0.005620418402196262),\n",
       " ('ticket', 0.005620418402196262),\n",
       " ('miller', 0.005536343241782649),\n",
       " ('laminator', 0.005412126655397238),\n",
       " ('camcorder', 0.005412126655397238),\n",
       " ('cenotaph', 0.005402784546721274),\n",
       " ('recorder', 0.00538623430210475),\n",
       " ('dunkirk', 0.005310132043292975),\n",
       " ('hostilities', 0.005310132043292975),\n",
       " ('cartridge', 0.005284346315251138),\n",
       " ('percussion', 0.0051520502020132395),\n",
       " ('metropolitan', 0.0051520502020132395),\n",
       " ('croatia', 0.005100565714829716),\n",
       " ('trophy', 0.005032710776429654),\n",
       " ('photographer', 0.005032710776429654),\n",
       " ('denmark', 0.005032710776429654),\n",
       " ('recollection', 0.005032710776429654),\n",
       " ('d-day', 0.005029726088305908),\n",
       " ('scenery', 0.004953570268963422),\n",
       " ('erection', 0.004870913989857514),\n",
       " ('azores', 0.004867621039685226),\n",
       " ('bomber', 0.00483312067223541),\n",
       " ('brussels', 0.004798964413316095),\n",
       " ('guernsey', 0.004798964413316095),\n",
       " ('tribute', 0.0047605280005077345),\n",
       " ('iceland', 0.004600307657087651),\n",
       " ('evacuation', 0.004600307657087651),\n",
       " ('recital', 0.004600307657087651),\n",
       " ('forties', 0.004496007241027754),\n",
       " ('sardinia', 0.004496007241027754),\n",
       " ('seats', 0.0044494979017387064),\n",
       " ('cone', 0.0044494979017387064),\n",
       " ('wages', 0.0044494979017387064),\n",
       " ('newsreel', 0.004431076585742018),\n",
       " ('ringer', 0.004430360616215793),\n",
       " ('blitz', 0.004420490286185753),\n",
       " ('huff', 0.004420460083681042),\n",
       " ('infirm', 0.0042778041599652065),\n",
       " ('out-of-school', 0.004215313801647197),\n",
       " ('piano', 0.004215313801647197),\n",
       " ('filing', 0.004215313801647197),\n",
       " ('rack', 0.004215313801647197),\n",
       " ('pottery', 0.004215313801647197),\n",
       " ('enlarge', 0.00408665244674855),\n",
       " ('aircrew', 0.004027600560196175),\n",
       " ('puff', 0.003982599032469731),\n",
       " ('skip', 0.003981129701555685),\n",
       " ('sketch', 0.003981129701555685),\n",
       " ('spain', 0.003981129701555685),\n",
       " ('stack', 0.003981129701555685),\n",
       " ('crockery', 0.003981129701555685),\n",
       " ('glenn', 0.003929297852160927),\n",
       " ('arizona', 0.003877192012524266),\n",
       " ('mixer', 0.0037884886587780664),\n",
       " ('pageant', 0.0037880243233249697),\n",
       " ('bunting', 0.003774533082322241),\n",
       " ('sikh', 0.003774533082322241),\n",
       " ('cent', 0.003774533082322241),\n",
       " ('batting', 0.0037404148575417915),\n",
       " ('bevin', 0.003540088028861983),\n",
       " ('palace', 0.003512761501372663),\n",
       " ('beaver', 0.003512761501372663),\n",
       " ('puppet', 0.003512761501372663),\n",
       " ('kettering', 0.003512761501372663),\n",
       " ('authentic', 0.003512761501372663),\n",
       " ('parochial', 0.003512761501372663),\n",
       " ('fixture', 0.003512761501372663),\n",
       " ('charter', 0.003512761501372663),\n",
       " ('poem', 0.003512761501372663),\n",
       " ('preserves', 0.003512761501372663),\n",
       " ('symphony', 0.003496637836915357),\n",
       " ('colt', 0.0034579366857103114),\n",
       " ('korean', 0.0034579366857103114),\n",
       " ('wreath', 0.00340037714321981),\n",
       " ('cd-rom', 0.003322359978449604),\n",
       " ('rehearse', 0.0032785774012811524),\n",
       " ('stove', 0.0032785774012811524),\n",
       " ('roundabout', 0.0032785774012811524),\n",
       " ('quarter', 0.0032712620046792757),\n",
       " ('soldier', 0.0032712620046792757),\n",
       " ('sack', 0.0032712620046792757),\n",
       " ('barbershop', 0.0032220804481569404),\n",
       " ('churchill', 0.003143578805191192),\n",
       " ('balloon', 0.003143578805191192),\n",
       " ('rally', 0.003143578805191192),\n",
       " ('infantry', 0.0030975770252542347),\n",
       " ('drought', 0.003060339428897829),\n",
       " ('float', 0.0030443933011896416),\n",
       " ('abseil', 0.0030443933011896416),\n",
       " ('plumbing', 0.0030443933011896416),\n",
       " ('invaluable', 0.0030443933011896416),\n",
       " ('occasion', 0.0030443933011896416),\n",
       " ('instrumental', 0.0030443933011896416),\n",
       " ('boston', 0.0030443933011896416),\n",
       " ('grandchild', 0.0030443933011896416),\n",
       " ('german', 0.003019626465857793),\n",
       " ('corridor', 0.003019626465857793),\n",
       " ('telescope', 0.003019626465857793),\n",
       " ('shredder', 0.003019626465857793),\n",
       " ('buoyancy', 0.002976669660468481),\n",
       " ('nostalgia', 0.0029532088697329815),\n",
       " ('wren', 0.0029532088697329815),\n",
       " ('handbell', 0.0029532088697329815),\n",
       " ('carpeting', 0.002913864864096131),\n",
       " ('leek', 0.002913864864096131),\n",
       " ('conductor', 0.002913864864096131),\n",
       " ('anderson', 0.002913864864096131),\n",
       " ('naples', 0.0028193203921373223),\n",
       " ('reimbursement', 0.0028193203921373223),\n",
       " ('nelson', 0.002810209201098131),\n",
       " ('permission', 0.002810209201098131),\n",
       " ('campaigning', 0.002810209201098131),\n",
       " ('uneven', 0.002810209201098131),\n",
       " ('sundries', 0.002810209201098131),\n",
       " ('upwards', 0.002810209201098131),\n",
       " ('rail', 0.002810209201098131),\n",
       " ('chess', 0.002810209201098131),\n",
       " ('dock', 0.002810209201098131),\n",
       " ('brigade', 0.002810209201098131),\n",
       " ('wildflower', 0.002810209201098131),\n",
       " ('harmony', 0.002810209201098131),\n",
       " ('astronomy', 0.002810209201098131),\n",
       " ('bicentennial', 0.002769422866088761)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = 0\n",
    "result = {w: tfids[index][vectorizer.vocabulary_.get(w)]\n",
    "          for w in vectorizer.get_feature_names()}\n",
    "sorted_by_value = sorted(result.items(), key=lambda kv: kv[1], reverse=True)\n",
    "sorted_by_value[0:199]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tfidfs(tfidfs, vectorizer, index):\n",
    "    result = {w: tfidfs[index][vectorizer.vocabulary_.get(w)]\n",
    "              for w in vectorizer.get_feature_names()}\n",
    "\n",
    "    result = {w: c for w, c in result.items() if c > 0}\n",
    "    normalizing_factor = max(c for c in result.values())\n",
    "\n",
    "    result = {w: c / normalizing_factor\n",
    "              for w, c in result.items()}\n",
    "    result = sorted(result.items(), key=lambda kv: kv[1], reverse=True)\n",
    "    result = result[:200]\n",
    "    \n",
    "    return dict((x,y) for x, y in result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\n",
      "0\n",
      "100\n",
      "award values\n",
      "0\n",
      "100\n",
      "benefs\n",
      "0\n",
      "100\n",
      "Processing done for 2005\n",
      "data\n",
      "0\n",
      "100\n",
      "award values\n",
      "0\n",
      "100\n",
      "benefs\n",
      "0\n",
      "100\n",
      "Processing done for 2006\n",
      "data\n",
      "0\n",
      "100\n",
      "award values\n",
      "0\n",
      "100\n",
      "benefs\n",
      "0\n",
      "100\n",
      "Processing done for 2007\n",
      "data\n",
      "0\n",
      "100\n",
      "award values\n",
      "0\n",
      "100\n",
      "benefs\n",
      "0\n",
      "100\n",
      "Processing done for 2008\n",
      "data\n",
      "0\n",
      "100\n",
      "award values\n",
      "0\n",
      "100\n",
      "benefs\n",
      "0\n",
      "100\n",
      "Processing done for 2009\n",
      "data\n",
      "0\n",
      "100\n",
      "award values\n",
      "0\n",
      "100\n",
      "benefs\n",
      "0\n",
      "100\n",
      "Processing done for 2010\n",
      "data\n",
      "0\n",
      "100\n",
      "award values\n",
      "0\n",
      "100\n",
      "benefs\n",
      "0\n",
      "100\n",
      "Processing done for 2011\n",
      "data\n",
      "0\n",
      "100\n",
      "award values\n",
      "0\n",
      "100\n",
      "benefs\n",
      "0\n",
      "100\n",
      "Processing done for 2012\n",
      "data\n",
      "0\n",
      "100\n",
      "award values\n",
      "0\n",
      "100\n",
      "benefs\n",
      "0\n",
      "100\n",
      "Processing done for 2013\n",
      "data\n",
      "0\n",
      "100\n",
      "award values\n",
      "0\n",
      "100\n",
      "benefs\n",
      "0\n",
      "100\n",
      "Processing done for 2014\n",
      "data\n",
      "0\n",
      "100\n",
      "award values\n",
      "0\n",
      "100\n",
      "benefs\n",
      "0\n",
      "100\n",
      "Processing done for 2015\n",
      "data\n",
      "0\n",
      "100\n",
      "award values\n",
      "0\n",
      "100\n",
      "benefs\n",
      "0\n",
      "100\n",
      "Processing done for 2016\n",
      "data\n",
      "0\n",
      "100\n",
      "award values\n",
      "0\n",
      "100\n",
      "benefs\n",
      "0\n",
      "100\n",
      "Processing done for 2017\n"
     ]
    }
   ],
   "source": [
    "json_output = {}\n",
    "for i, (year, text) in enumerate(data):\n",
    "    json_output[year] = {}\n",
    "    result = process_tfidfs(tfids, vectorizer, i)\n",
    "    award_data_vals = award_data(result.keys(),year)\n",
    "    with codecs.open('output/{}-awarddata.json'.format(year), 'w', encoding='utf-8') as f:\n",
    "      f.write(json.dumps(award_data_vals, ensure_ascii=False))\n",
    "    json_output[year]['award_data'] = award_data_vals\n",
    "    award_vals = award_values(result.keys(),year)\n",
    "    json_output[year]['award'] = award_vals\n",
    "    benec_vals = beneficiaries(result.keys(),year)\n",
    "    json_output[year]['benefactors'] = benec_vals\n",
    "    result = {w: tfids[i][vectorizer.vocabulary_.get(w)]\n",
    "          for w in vectorizer.get_feature_names() if tfids[i][vectorizer.vocabulary_.get(w)] > 0 }\n",
    "    json_output[year]['word_values'] = result\n",
    "\n",
    "    with codecs.open('output/{}-awards.json'.format(year), 'w', encoding='utf-8') as f:\n",
    "      f.write(json.dumps(award_vals, ensure_ascii=False))\n",
    "    with codecs.open('output/{}-benecs.json'.format(year), 'w', encoding='utf-8') as f:\n",
    "      f.write(json.dumps(benec_vals, ensure_ascii=False))\n",
    "    with codecs.open('output/{}-words.json'.format(year), 'w', encoding='utf-8') as f:\n",
    "      f.write(json.dumps(result, ensure_ascii=False))    \n",
    "\n",
    "    print('Processing done for {}'.format(year))\n",
    "\n",
    "with codecs.open('output/alljson.json'.format(year), 'w', encoding='utf-8') as f:\n",
    "  f.write(json.dumps(json_output, ensure_ascii=False))    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
